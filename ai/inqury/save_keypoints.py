import os
from dotenv import load_dotenv
from pymongo import MongoClient
import gspread
from google.oauth2.service_account import Credentials
from google.auth import default

import mediapipe as mp
import base64
import cv2
import numpy as np

load_dotenv()
mongo_db_url = os.getenv("MONGO_DB_URL")
client = MongoClient(mongo_db_url)
db = client["dev"]
json_key_file = os.getenv("SPREADSHEET_JSON_KEY")

sign_language_collection = db["sign_language"]

name_url_dict = {
    doc["name"]: doc["url"]
    for doc in sign_language_collection.find({}, {"_id": 0, "name": 1, "url": 1})
}

DATA_PATH = "angles"

mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

def process_frame(image_data):
    image_bytes = base64.b64decode(image_data)
    np_arr = np.frombuffer(image_bytes, np.uint8)
    img = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)

    return img

def extract_keypoints(results):
    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in
                     results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 4)
    lh = np.array([[res.x, res.y, res.z] for res in
                   results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)
    rh = np.array([[res.x, res.y, res.z] for res in
                   results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(
        21 * 3)
    return np.concatenate([pose, lh, rh])

"""
ê°ë„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
"""
def calculate_angle(a, b, c):
    a = np.array(a)
    b = np.array(b)
    c = np.array(c)

    ba = a - b
    bc = c - b

    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)
    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))
    return np.degrees(angle)

"""
angle ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
- íŒ”, ë‹¤ë¦¬, ìƒì²´ë¥¼ ê³„ì‚°í•¨
"""

def extract_angles(results):
    if not results.pose_landmarks:
        return np.zeros(6)  # 6ê°œ ê°ë„ ê¸°ë³¸ê°’

    landmarks = results.pose_landmarks.landmark
    get_coord = lambda idx: [landmarks[idx].x, landmarks[idx].y, landmarks[idx].z]

    angles = []

    # íŒ”
    angles.append(calculate_angle(get_coord(11), get_coord(13), get_coord(15)))  # ì™¼íŒ”
    angles.append(calculate_angle(get_coord(12), get_coord(14), get_coord(16)))  # ì˜¤ë¥¸íŒ”

    # ë‹¤ë¦¬
    angles.append(calculate_angle(get_coord(23), get_coord(25), get_coord(27)))  # ì™¼ë‹¤ë¦¬
    angles.append(calculate_angle(get_coord(24), get_coord(26), get_coord(28)))  # ì˜¤ë¥¸ë‹¤ë¦¬

    # ìƒì²´
    angles.append(calculate_angle(get_coord(11), get_coord(23), get_coord(25)))  # ì™¼ìƒì²´
    angles.append(calculate_angle(get_coord(12), get_coord(24), get_coord(26)))  # ì˜¤ë¥¸ìƒì²´

    return np.array(angles)


"""
Mediapipe Hands Model ì„¤ì •

- ì¸ì‹í• ìˆ˜ ìˆëŠ” ì†ì˜ ê°¯ìˆ˜ ì„ ì–¸ : ì–‘ì†
- ì‚¬ìš©í•  ì†ì˜ ìœ í‹¸ë¦¬í‹° ì„ ì–¸ : hands ìœ í‹¸ë¦¬í‹°
- íšŒì†Œ íƒì§€ ì‹ ë¢°ë„, ìµœì†Œ ì¶”ì  ì‹ ë¢°ë„ (ê¸°ë³¸ê°’ ì‚¬ìš©) : 0.5
"""
# MAX_NUM_HANDS = 2
mp_hands=mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

"""
Mongo DBì— ìˆëŠ” ëª¨ë“  ë™ì˜ìƒë“¤ ì¶”ì¶œ
- ì²˜ë¦¬ ì¤‘ì— ì„±ëŠ¥ìœ¼ë¡œ ì¸í•œ ì˜ìƒ ëˆ„ë½ì´ ë°œìƒí•¨ í™•ì¸ -> ì•„ë˜ ì½”ë“œì—ì„œ ëŒ€ì²˜
"""

"""
ë‘ ì†ì˜ anglesë¡œ í•™ìŠµìš© ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ë•Œ
"""
seq_length = 90
os.makedirs('angles', exist_ok=True)

for action, url in name_url_dict.items():
    cap = cv2.VideoCapture(url)
    if not cap.isOpened():
        print(f"ë¹„ë””ì˜¤ ì—´ê¸° ì˜¤ë¥˜: {url}")
        continue

    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps if fps > 0 else 0

    print(f"\n[Action: {action}]")
    print(f"FPS: {fps}")
    print(f"ì´ í”„ë ˆì„ ìˆ˜: {total_frames}")
    print(f"ì´ ì¬ìƒ ì‹œê°„: {duration:.2f}ì´ˆ")

    if total_frames < seq_length:
        print(f"âš ï¸ ë¹„ë””ì˜¤ í”„ë ˆì„ ìˆ˜ ë¶€ì¡± ({seq_length}í”„ë ˆì„ ë¯¸ë§Œ): {url}")
        cap.release()
        continue

    start_frame = total_frames - seq_length
    selected_frames = np.arange(start_frame, total_frames, dtype=int)

    data = []
    frame_idx = 0

    with mp_hands.Hands(
        static_image_mode=False,
        max_num_hands=2,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    ) as hands:

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            img = cv2.flip(frame, 1)
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            result = hands.process(img_rgb)

            if result.multi_hand_landmarks:
                for res in result.multi_hand_landmarks:
                    joint = np.zeros((21, 3))
                    for j, lm in enumerate(res.landmark):
                        joint[j] = [lm.x, lm.y, lm.z]

                    v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :]
                    v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :]
                    v = v2 - v1
                    v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]

                    angle = np.arccos(np.einsum('nt,nt->n',
                        v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], 
                        v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) 
                    angle = np.degrees(angle)

                    angle_label = np.array([angle], dtype=np.float32)
                    angle_label = np.append(angle_label, frame_idx)
                    d = np.concatenate([joint.flatten(), angle_label])
                    data.append(d)

            if len(data) >= seq_length:
                break

            frame_idx += 1


    cap.release()
    cv2.destroyAllWindows()

    data = np.array(data)

    # # âš ï¸ ë°ì´í„° ìˆ˜ê°€ ë¶€ì¡±í•œ ê²½ìš°, ì œë¡œ íŒ¨ë”© ì¶”ê°€
    # if len(data) < seq_length:
    #     pad_len = seq_length - len(data)
    #     pad_data = np.zeros((pad_len, data.shape[1]))
    #     data = np.vstack([pad_data, data])
    #     print(f"ğŸ“Œ ë¶€ì¡±í•œ {pad_len}ê°œì˜ í”„ë ˆì„ì„ 0ìœ¼ë¡œ íŒ¨ë”© ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")

    # ë°ì´í„° ìˆ˜ê°€ ë¶€ì¡±í•œ ê²½ìš°, ì§ì „ í”„ë ˆì„ì˜ ë°˜ë³µ
    if len(data) < seq_length:
        pad_len = seq_length - len(data)
        if len(data) > 0:
            pad_data = np.tile(data[-1], (pad_len, 1))  # ë§ˆì§€ë§‰ í”„ë ˆì„ ë°˜ë³µ
        else:
            pad_data = np.zeros((pad_len, 21*3 + 15 + 1))  # ì•„ë¬´ ë°ì´í„°ë„ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ íŒ¨ë”©
        data = np.vstack([pad_data, data])
        print(f"ğŸ“Œ ë¶€ì¡±í•œ {pad_len}ê°œì˜ í”„ë ˆì„ì„ ë§ˆì§€ë§‰ í”„ë ˆì„ìœ¼ë¡œ íŒ¨ë”©í–ˆìŠµë‹ˆë‹¤.")


    # ì‹œí€€ìŠ¤ ìƒì„±
    full_seq_data = []
    for seq in range(len(data) - seq_length + 1):
        full_seq_data.append(data[seq:seq + seq_length])
    full_seq_data = np.array(full_seq_data)

    print(f"[{action}] shape: {full_seq_data.shape}")
    np.save(os.path.join('angles', f'{action}.npy'), full_seq_data)

    
    # # í‚¤í¬ì¸íŠ¸ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•œ ì½”ë“œ
    
    # keypoints_list = []
    # with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
    #     frame_idx = 0
    #     selected_idx = 0

    #     while True:
    #         ret, frame = cap.read()
    #         if not ret:
    #             break

    #         if frame_idx == selected_frames[selected_idx]:
    #             img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    #             results = holistic.process(img_rgb)
    #             angles = extract_angles(results)  # âœ… ì—¬ê¸°!
    #             keypoints_list.append(angles)

    #             selected_idx += 1
    #             if selected_idx >= len(selected_frames):
    #                 break

    #         frame_idx += 1


    # cap.release()

    # # ì €ì¥
    # output_dir = os.path.join(DATA_PATH, action)
    # os.makedirs(output_dir, exist_ok=True)

    # for frame_num, keypoint in enumerate(keypoints_list):
    #     np.save(os.path.join(output_dir, f"{frame_num}.npy"), keypoint)

    # print(f"[{action}] ëª¨ë“  í‚¤í¬ì¸íŠ¸ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤!")
    # with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
    #     frame_idx = 0
    #     selected_idx = 0

    #     while True:
    #         ret, frame = cap.read()
    #         if not ret:
    #             break

    #         if frame_idx == selected_frames[selected_idx]:
    #             img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    #             results = holistic.process(img_rgb)
    #             keypoints = extract_keypoints(results)
    #             keypoints_list.append(keypoints)

    #             selected_idx += 1
    #             if selected_idx >= len(selected_frames):
    #                 break

    #         frame_idx += 1

    # cap.release()

    # # ì €ì¥
    # output_dir = os.path.join(DATA_PATH, action)
    # os.makedirs(output_dir, exist_ok=True)

    # for frame_num, keypoint in enumerate(keypoints_list):
    #     np.save(os.path.join(output_dir, f"{frame_num}.npy"), keypoint)

    # print(f"[{action}] ëª¨ë“  í‚¤í¬ì¸íŠ¸ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤!")



""""
ëˆ„ë½ëœ ë™ì˜ìƒ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
â—ëˆ„ë½ëœ ë™ì‘ì´ ìˆìŠµë‹ˆë‹¤:
- ì˜ìˆ˜ì¦
- ì•ˆë…•í•˜ì„¸ìš”,ì•ˆë…•íˆ ê°€ì‹­ì‹œì˜¤
- ìƒí¬ë¦¼, íœ˜í•‘í¬ë¦¼
- ë¶€ë“œëŸ½ë‹¤
"""
# url = "https://drive.google.com/uc?export=download&id=1XVHkBiC7G5eH1ftbXx0vNMsOuxdadfEe"

# seq_length = 100
# url = "http://sldict.korean.go.kr/multimedia/multimedia_files/convert/20220811/1009678/MOV000359988_700X466.mp4"
# action = "ì•ˆë…•í•˜ì„¸ìš”,ì•ˆë…•íˆ ê°€ì‹­ì‹œì˜¤"
# cap = cv2.VideoCapture(url)
# if not cap.isOpened():
#     print(f"ë¹„ë””ì˜¤ ì—´ê¸° ì˜¤ë¥˜: {url}")
#     exit()

# fps = cap.get(cv2.CAP_PROP_FPS)
# total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
# duration = total_frames / fps if fps > 0 else 0

# print(f"\n[Action: {action}]")
# print(f"FPS: {fps}")
# print(f"ì´ í”„ë ˆì„ ìˆ˜: {total_frames}")
# print(f"ì´ ì¬ìƒ ì‹œê°„: {duration:.2f}ì´ˆ")

# if total_frames < seq_length:
#     print(f"âš ï¸ ë¹„ë””ì˜¤ í”„ë ˆì„ ìˆ˜ ë¶€ì¡± ({seq_length}í”„ë ˆì„ ë¯¸ë§Œ): {url}")
#     cap.release()
#     exit()

# start_frame = total_frames - seq_length
# selected_frames = np.arange(start_frame, total_frames, dtype=int)

# data = []
# selected_idx = 0
# frame_idx = 0

# with mp_hands.Hands(
#     static_image_mode=False,
#     max_num_hands=2,
#     min_detection_confidence=0.5,
#     min_tracking_confidence=0.5
# ) as hands:

#     while True:
#         ret, frame = cap.read()
#         if not ret:
#             break

#         if selected_idx < len(selected_frames) and frame_idx == selected_frames[selected_idx]:
#             img = cv2.flip(frame, 1)
#             img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
#             result = hands.process(img_rgb)

#             # print(f"í”„ë ˆì„ {frame_idx} ê²€ì‚¬ ì¤‘...")

#             if result.multi_hand_landmarks:
#                 # print(f"â¡ï¸ ì† ê²€ì¶œë¨! í”„ë ˆì„: {frame_idx}")
#                 for res in result.multi_hand_landmarks:
#                     joint = np.zeros((21, 3))
#                     for j, lm in enumerate(res.landmark):
#                         joint[j] = [lm.x, lm.y, lm.z]

#                     v1 = joint[[0, 1, 2, 3, 0, 5, 6, 7, 0, 9, 10, 11, 0, 13, 14, 15, 0, 17, 18, 19], :]
#                     v2 = joint[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], :]
#                     v = v2 - v1
#                     v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]

#                     angle = np.arccos(np.einsum('nt,nt->n',
#                                                 v[[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 16, 17, 18], :],
#                                                 v[[1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 17, 18, 19], :]))
#                     angle = np.degrees(angle)
#                     angle_label = np.array([angle], dtype=np.float32)
#                     angle_label = np.append(angle_label, frame_idx)
#                     d = np.concatenate([joint.flatten(), angle_label])
#                     data.append(d)

#             selected_idx += 1
#             if selected_idx >= len(selected_frames):
#                 break

#         frame_idx += 1

# cap.release()
# cv2.destroyAllWindows()

# data = np.array(data)

# # âš ï¸ ë°ì´í„° ìˆ˜ê°€ ë¶€ì¡±í•œ ê²½ìš°, ì œë¡œ íŒ¨ë”© ì¶”ê°€
# if len(data) < seq_length:
#     pad_len = seq_length - len(data)
#     pad_data = np.zeros((pad_len, data.shape[1]))
#     data = np.vstack([pad_data, data])
#     print(f"ğŸ“Œ ë¶€ì¡±í•œ {pad_len}ê°œì˜ í”„ë ˆì„ì„ 0ìœ¼ë¡œ íŒ¨ë”© ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")

# # ì‹œí€€ìŠ¤ ìƒì„±
# full_seq_data = []
# for seq in range(len(data) - seq_length + 1):
#     full_seq_data.append(data[seq:seq + seq_length])
# full_seq_data = np.array(full_seq_data)


# print(f"[{action}] shape: {full_seq_data.shape}")
# np.save(os.path.join('angles_after_pad', f"seq_{action}.npy"), full_seq_data)

# # í‚¤í¬ì¸íŠ¸ ì „ìš©
# cap = cv2.VideoCapture(url)
# if not cap.isOpened():
#     print(f"ë¹„ë””ì˜¤ ì—´ê¸° ì˜¤ë¥˜: {url}")
#     cap.release()
#     exit()

# fps = cap.get(cv2.CAP_PROP_FPS)
# total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
# duration = total_frames / fps if fps > 0 else 0

# print(f"\n[Action: {action}]")
# print(f"FPS: {fps}")
# print(f"ì´ í”„ë ˆì„ ìˆ˜: {total_frames}")
# print(f"ì´ ì¬ìƒ ì‹œê°„: {duration:.2f}ì´ˆ")

# if total_frames < 90:
#     print(f"âš ï¸ ë¹„ë””ì˜¤ í”„ë ˆì„ ìˆ˜ ë¶€ì¡± (90í”„ë ˆì„ ë¯¸ë§Œ): {url}")
#     print(f"ë¶€ì¡±í–ˆë˜ í”„ë ˆì„ ìˆ˜ : {total_frames}")
#     cap.release()
#     exit()

# # ë§ˆì§€ë§‰ 90í”„ë ˆì„ ì„ íƒ
# start_frame = total_frames - 90
# selected_frames = np.arange(start_frame, total_frames, dtype=int)

# keypoints_list = []
# with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
#     frame_idx = 0
#     selected_idx = 0

#     while True:
#         ret, frame = cap.read()
#         if not ret:
#             break

#         if frame_idx == selected_frames[selected_idx]:
#             img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
#             results = holistic.process(img_rgb)
#             keypoints = extract_keypoints(results)
#             keypoints_list.append(keypoints)

#             selected_idx += 1
#             if selected_idx >= len(selected_frames):
#                 break

#         frame_idx += 1

# cap.release()

# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
#     frame_idx = 0
#     selected_idx = 0

#     while True:
#         ret, frame = cap.read()
#         if not ret:
#             break

#         if frame_idx == selected_frames[selected_idx]:
#             img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
#             results = holistic.process(img_rgb)
#             angles = extract_angles(results)  # âœ… ì—¬ê¸°!
#             keypoints_list.append(angles)

#             selected_idx += 1
#             if selected_idx >= len(selected_frames):
#                 break

#         frame_idx += 1


# cap.release()

# # ì €ì¥
# output_dir = os.path.join(DATA_PATH, action)
# os.makedirs(output_dir, exist_ok=True)

# for frame_num, keypoint in enumerate(keypoints_list):
#     np.save(os.path.join(output_dir, f"{frame_num}.npy"), keypoint)

# print(f"[{action}] ëª¨ë“  í‚¤í¬ì¸íŠ¸ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤!")


"""
ëˆ„ë½ëœ ë™ì‘ ìœ ë¬´ í™•ì¸ - í‚¤í¬ì¸íŠ¸
"""
# saved_actions = set([
#     f for f in os.listdir(DATA_PATH)
#     if os.path.isdir(os.path.join(DATA_PATH, f))
# ])

# # 2. DB ë˜ëŠ” name_url_dictì— ìˆëŠ” ì‹¤ì œ ë™ì‘ ì´ë¦„
# expected_actions = set(name_url_dict.keys())

# # 3. ëˆ„ë½ëœ ë™ì‘ ëª©ë¡
# missing_actions = expected_actions - saved_actions

# # 4. ì¶œë ¥
# if missing_actions:
#     print("â—ëˆ„ë½ëœ ë™ì‘ì´ ìˆìŠµë‹ˆë‹¤:")
#     for action in missing_actions:
#         print("-", action)
# else:
#     print("âœ… ëª¨ë“  ë™ì‘ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")


"""
ëˆ„ë½ëœ ë™ì‘ ìœ ë¬´ í™•ì¸ - angle
â—ëˆ„ë½ëœ ë™ì‘ì´ ìˆìŠµë‹ˆë‹¤:
- ìƒí¬ë¦¼, íœ˜í•‘í¬ë¦¼
- ì•ˆë…•í•˜ì„¸ìš”,ì•ˆë…•íˆ ê°€ì‹­ì‹œì˜¤
"""
# # 1. ì €ì¥ëœ ì•¡ì…˜: angles/ ë””ë ‰í† ë¦¬ ì•ˆì˜ íŒŒì¼ëª…ì—ì„œ ë™ì‘ ì´ë¦„ ì¶”ì¶œ
# saved_actions = set([
#     filename.replace("seq_", "").replace(".npy", "")
#     for filename in os.listdir('angles_after_pad')
#     if filename.endswith(".npy")
# ])

# # 2. ê¸°ëŒ€ë˜ëŠ” ì•¡ì…˜ ëª©ë¡
# expected_actions = set(name_url_dict.keys())

# # 3. ëˆ„ë½ëœ ë™ì‘
# missing_actions = expected_actions - saved_actions

# # 4. ì¶œë ¥
# if missing_actions:
#     print("â—ëˆ„ë½ëœ ë™ì‘ì´ ìˆìŠµë‹ˆë‹¤:")
#     for action in sorted(missing_actions):
#         print("-", action)
# else:
#     print("âœ… ëª¨ë“  ë™ì‘ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")


"""
ë™ì‘ì˜ ê°¯ìˆ˜ í™•ì¸
"""
# folders = [f for f in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, f))]
# print(f"ì´ {len(folders)}ê°œì˜ ë™ì‘ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")